{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to use Keras to implement a simple single-layer (shallow network) model using handcrafted features as input.\n",
    "Try to follow, understand the code and do the exercises/answer the questions when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "import keras_models\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "At this point we suppose that you've read the README.md file and that you have downloaded the dataset. \n",
    "Move the files train.csv, test.csv and test_labels.csv inside the folder portoai-nlp.\n",
    "\n",
    "It's important to follow some mandatory deep learning rules when developing models:\n",
    " - We split the data in (at least) 3 parts: training, validation and testing\n",
    " - While we train out models on the training data, we use the validation split to see how it is performing\n",
    " - We **do not** look at the results in the test set. You have to think of the test set as 'what I will encounter in the real-world' and therefore you should only run your model on it when you are done training and confident with your score in the validation set\n",
    " \n",
    "Remember that the model used in 'real-world' will probably not have seen that data before, and therefore we **do not** take any information from the testing set to the training set, or else we will be cheating. During development, we make the assumption that the validation set will be similar to the testing set, and that improvements on validation will be a positive thing.\n",
    "\n",
    "Make sure you understand the importance of this train/val/test split before going any further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data, fill empty comments with empty strings ' '\n",
    "N_ROWS = 30000\n",
    "dataset = pd.read_csv('train.csv', nrows=N_ROWS).fillna(' ')\n",
    "LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a subset of the whole dataset. \n",
    "\n",
    "**At the end of running the exercises**, feel free to try with a bigger `N_ROWS` variable to see the difference in results. You have a maximum of 159571 rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having a look at the data\n",
    "We can see that we have 8 columns in the dataset: the row identifier (id), comment_text and the other 6 labels we want to predict (toxic, severe_toxic, obscene, threat, insult and identity_hate). \n",
    "\n",
    "Note we want to predict all these labels having only the comment_text.\n",
    "\n",
    "Note also that a comment text can 0 up to 6 labels of toxicity. We could train a different classifier for each label but we will take advantage of models that predict all of them at the same time, taking advantage of things such as: a 'severe_toxic' comment might as well be a 'toxic' comment, right? The model will try to learn these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validation split\n",
    "We will randomly assign 80% of the dataset to train and the rest of the 20% to validation. Note that the test set is already in another file, `test.csv` that will not be used during model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(dataset)) < 0.8\n",
    "train = dataset[msk]\n",
    "val = dataset[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size:\", train.shape[0], \", Val size:\", val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the comment text from the train and test dataframes\n",
    "train_text = train['comment_text']\n",
    "val_text = val['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to transform our text into something that we can feed a machine model with, which is numbers!\n",
    "\n",
    "We will start with a simple count of words using [sklearn's CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). This will simply convert each comment text to a **fixed length** vector with the dimension of the vocabulary (vocabulary is the set of all words that appear in the training set).\n",
    "\n",
    "You can see in [sklearn's example](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) how this is done. \n",
    "\n",
    "For example, for the comment 'I love NLP', the resulting vector will be filled with zeros, except for the positions of the vector corresponding to the words 'I', 'love' and  'NLP' which will have a 1, since they appear one time on that string. It's nothing more than a word count. Bag of words!\n",
    "\n",
    "Since we have a lot of words in our vocabulary, we will reduce the vector dimension by passing a MAX_LENGTH number to the `max_features` function argument, that will make sure the CountVectorizer will only construct a vector for each string considering the MAX_LENGTH most frequent words in the training set. Once again, feel free to play with these parameters at the end of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1000\n",
    "word_vectorizer = CountVectorizer(stop_words='english', max_features=MAX_LENGTH)\n",
    "word_vectorizer.fit(train_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "val_word_features = word_vectorizer.transform(val_text)\n",
    "\n",
    "y_train = train[LABELS].values \n",
    "y_val = val[LABELS].values\n",
    "\n",
    "print(train_word_features.shape, val_word_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our input features\n",
    "train_word_features[:5, :].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed a basic linear model with the features\n",
    "Check the function `get_shallow_features_model` under `keras_models.py` to see how to build a simple linear model in Keras.\n",
    "\n",
    "Can you explain this line in here `output = Dense(6, activation='sigmoid')(input)`? Understand this before you continue any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "bow_model = keras_models.get_shallow_features_model(number_feats=train_word_features.shape[1])\n",
    "bow_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output above you can see that we have just constructed a simple shallow network with a single layer with an output shape of 6.\n",
    "\n",
    "We will now train the model with out example from the train set during 10 epochs, which means that we will do feed the networks the same data 5 times and, at the end of each loop, we will measure how well it performs in our validation set.\n",
    "\n",
    "While runnning the code below you can see the accuracy (val_acc) and [ROC-AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (val_auc) being printed at the end of each training loop/epoch.\n",
    "\n",
    "Feel free to increase the number of epochs and see the difference in score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check https://keras.io/models/model/#fit for more option on arguments to pass to the fit function\n",
    "history = bow_model.fit(train_word_features, y_train, batch_size=128, epochs=10, validation_data=(val_word_features, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = [x for x in range(len(history.history['auc']))]\n",
    "trace1 = {\"y\": history.history['auc'], \"x\": epochs, \"name\": \"train_auc\", \"type\": \"scatter\"}\n",
    "trace2 = {\"y\": history.history['val_auc'], \"x\": epochs, \"name\": \"val_auc\", \"type\": \"scatter\"}\n",
    "trace3 = {\"y\": history.history['loss'], \"x\": epochs, \"name\": \"train_loss\", \"type\": \"scatter\"}\n",
    "trace4 = {\"y\": history.history['val_loss'], \"x\": epochs, \"name\": \"val_loss\", \"type\": \"scatter\"}\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "layout = {\"title\": \"BoW model\"}\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='bow-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above you can see the evolution of loss and ROC-AUC in the train and val set.\n",
    "\n",
    "While training neural networks, it's very useful to closer look at these plots to see if we are overfitting or even underfitting. \n",
    "\n",
    "For overfitting cases, the training loss continues to decrease while the validation score does not increase (and the val loss does not decrease either). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now ask you to construct a different kind of input to the model, using [sklearn's TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) that constructs a vector of TF-IDF features for each comment text.\n",
    "\n",
    "These TF-IDF features work a bit different from the classic Bag of words, by giving more weight to words that appear in less documents (training examples).                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1000\n",
    "raise NotImplementedError(\"Implement TF-IDF features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the TF-IDF features, we will run our linear model on them.\n",
    "\n",
    "You will probably notice that with the training takes longer to get as good results as the BoW model. Try to increse the epochs to see the difference. \n",
    "\n",
    "A main reason why the BoW model converged faster is because we did not normalize our input to the network. That topic is out of scope of this tutorial, but feel free to read more about it in links like [this one](https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Solution to implement TF-IDF features:**\n",
    "```\n",
    "word_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "word_vectorizer.fit(train_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "val_word_features = word_vectorizer.transform(val_text)\n",
    "\n",
    "y_train = train[LABELS] \n",
    "y_val = val[LABELS]\n",
    "\n",
    "print(train_word_features.shape, val_word_features.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = keras_models.get_shallow_features_model(number_feats=train_word_features.shape[1])\n",
    "tfidf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/models/model/#fit\n",
    "history = tfidf_model.fit(train_word_features, y_train, batch_size=128, epochs=10, validation_data=(val_word_features, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [x for x in range(len(history.history['auc']))]\n",
    "trace1 = {\"y\": history.history['auc'], \"x\": epochs, \"name\": \"train_auc\", \"type\": \"scatter\"}\n",
    "trace2 = {\"y\": history.history['val_auc'], \"x\": epochs, \"name\": \"val_auc\", \"type\": \"scatter\"}\n",
    "trace3 = {\"y\": history.history['loss'], \"x\": epochs, \"name\": \"train_loss\", \"type\": \"scatter\"}\n",
    "trace4 = {\"y\": history.history['val_loss'], \"x\": epochs, \"name\": \"val_loss\", \"type\": \"scatter\"}\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "layout = {\"title\": \"TF-IDF model\"}\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='tfidf-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement your own features\n",
    "We now challenge you to implement your own features. \n",
    "\n",
    "Feel free to play with parameters of CountVectorizer, TfidfVectorizer and to add other features.\n",
    "\n",
    "For example, adding a 'text length' feature:\n",
    "```\n",
    "text_len_feat = np.expand_dims(train_text.apply(lambda x: len(x)).to_numpy(), 1)\n",
    "train_word_features = np.hstack((train_word_features.todense(), text_len_feat))\n",
    "text_len_feat = np.expand_dims(val_text.apply(lambda x: len(x)).to_numpy(), 1)\n",
    "val_word_features = np.hstack((val_word_features.todense(), text_len_feat))\n",
    "```\n",
    "\n",
    "Try to beat the previous models with new input features and run your model on the final Kaggle test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final test set\n",
    "test = pd.read_csv('test.csv').fillna(' ')\n",
    "raise NotImplementedError(\"Implement your own features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_model = keras_models.get_shallow_features_model(number_feats=train_word_features.shape[1])\n",
    "history = your_model.fit(train_word_features, y_train, batch_size=128, epochs=10, validation_data=(val_word_features, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [x for x in range(len(history.history['auc']))]\n",
    "trace1 = {\"y\": history.history['auc'], \"x\": epochs, \"name\": \"train_auc\", \"type\": \"scatter\"}\n",
    "trace2 = {\"y\": history.history['val_auc'], \"x\": epochs, \"name\": \"val_auc\", \"type\": \"scatter\"}\n",
    "trace3 = {\"y\": history.history['loss'], \"x\": epochs, \"name\": \"train_loss\", \"type\": \"scatter\"}\n",
    "trace4 = {\"y\": history.history['val_loss'], \"x\": epochs, \"name\": \"val_loss\", \"type\": \"scatter\"}\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "layout = {\"title\": \"Your model\"}\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='your-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get score in Kaggle's test set\n",
    "keras_models.get_score_on_kaggle_test_set(test_word_features, your_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First deep model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first assignment, we will ask you to follow the example of the shallow network from `keras_models.get_shallow_features_model` and implement a **deep** network now, making it a multilayer perceptron.\n",
    "\n",
    "Go to keras_models.py and implement the `get_MLP_features_model` function.\n",
    "\n",
    "Look at the image below to have an idea of how many changes you need to do to the previous network to get your first deep network running!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![this](https://qph.fs.quoracdn.net/main-qimg-8a19e73bffab9a7f6eab55fd5b47c00a.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing it, try running the models again through the deep network to see if you get better results!\n",
    "\n",
    "**(Possible) Solution:**\n",
    "\n",
    "```\n",
    "def get_MLP_features_model(number_feats):\n",
    "    # Inputs\n",
    "    input = Input(shape=[number_feats], name=\"input_features\")\n",
    "\n",
    "    x = Dense(256, activation='relu')(input)  # we can tweak the number of neurons and choose another activation\n",
    "    x = Dropout(0.2)(x)  # we can tweak the dropout value\n",
    "\n",
    "    # output\n",
    "    output = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "    # model\n",
    "    model = Model([input], output)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer='adam', metrics=['accuracy', auc])\n",
    "    return model\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
