{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from keras.preprocessing import text, sequence\n",
    "import pandas as pd\n",
    "import keras_models\n",
    "from scipy.sparse import hstack\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data, fill empty comments with empty strings ' '\n",
    "N_ROWS = 50000\n",
    "dataset = pd.read_csv('train.csv', nrows=N_ROWS).fillna(' ')\n",
    "LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "msk = np.random.rand(len(dataset)) < 0.8\n",
    "train = dataset[msk]\n",
    "val = dataset[~msk]\n",
    "y_train = train[LABELS].values \n",
    "y_val = val[LABELS].values\n",
    "print(\"Train size:\", train.shape[0], \", Val size:\", val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to embeddings\n",
    "In this tutorial we will have a totally different approach from the previous one: let's try to do the minimum feature engineering possible and focus on architecture engineering. \n",
    "\n",
    "This means that we will no longer handcraft features from text but will let the network learn the best representations for them. Word embeddings come handy in here.\n",
    "\n",
    "As you previously learned, word embeddings are a way to represent words in a vector with a fixed dimension, that is able serve as input to a neural network. When trained correctly, and with enough examples, the embeddings will be able to provide meaningful information to the network.\n",
    "\n",
    "![this](https://cdn-images-1.medium.com/max/2400/1*sXNXYfAqfLUeiDXPCo130w.png)\n",
    "\n",
    "Below you can visualize how a distributed representation with word embeddings can be way more meaningful than just a one-hot representation (sort of what you end up having in the Bag of Words approach).\n",
    "\n",
    "![this2](https://i.imgur.com/goEdlez.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "In practice, word embeddings are nothing more than a lookup table. \n",
    "\n",
    "In the example below you can see how two words are converted to word embeddings by using the embedding matrix: both `ride` and `a` are indexed in certain positions of the embedding table. All we have to do is to get those vectors corresponding to those positions and feed them to the network. At the end of the forward pass, the network will train these embeddings as 'normal' weights of the network and will update them in order to minimize the loss. \n",
    "\n",
    "![this3](https://image.slidesharecdn.com/kpisummerschool2015wordembeddingsandneurallanguagemodeling1-150828091027-lva1-app6892/95/kpi-summer-school-2015-word-embeddings-and-neural-language-modeling-28-638.jpg?cb=1440753116)\n",
    "\n",
    "Now the only feature engineering we have to do (if we can call it that way), is to change the words to numbers so that they will point to different indexes on the embedding table.\n",
    "\n",
    "Keras' tokenizer does that job for us, and we only have to specify how many words we want to represent (`num_words`). This `num_words` will define how many entries we have in our embedding matrix. The most frequent words will picked first, until `num_words` is reached, the rest will have the same index that will correspond to an unknown word. It's useful to always have an 'unknown word index' in our embedding matrix to deal with words that are not indexes in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 30000  # max words to represent\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=num_words) # setup tokenizer\n",
    "tokenizer.fit_on_texts(train['comment_text']) # create word_to_id  dictionary  (notice how we use only train data! no cheating!)\n",
    "train_words_indexes = tokenizer.texts_to_sequences(train['comment_text'])  # get words ids\n",
    "val_words_indexes = tokenizer.texts_to_sequences(val['comment_text']) # get words ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare input before and after tokenizer\n",
    "print(train['comment_text'].values[1])  # before tokenizer\n",
    "print(train_words_indexes[1])  # after tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with standard feedforward neural networks is that we need to feed them a fixed length input. \n",
    "\n",
    "There are many ways to solve this problem. In here will we'll take the simpler solution, which is to define a fixed limited to text string, `maxlen`. Strings will less than `maxlen` words will be padded with zeros and the words with more than `maxlen` words will be truncated to that size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100  # max number of words in a comment input\n",
    "\n",
    "train_words_indexes = sequence.pad_sequences(train_words_indexes, maxlen=maxlen, padding='post')\n",
    "val_words_indexes = sequence.pad_sequences(val_words_indexes, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how every sample now has the same length and that 0s were padded to it\n",
    "print(train_words_indexes.shape)\n",
    "train_words_indexes[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our input ready to pass to the neural network.\n",
    "\n",
    "All we have to do is to call the `keras_models.get_embeddings_model` function and define what dimension we want our embeddings to have, `embeddings_dim`. Feel free to play with the parameters and see what differences it makes to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 100\n",
    "\n",
    "# Construct model\n",
    "emb_model = keras_models.get_embeddings_model(maxlen, num_words, embeddings_dim)\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check https://keras.io/models/model/#fit for more option on arguments to pass to the fit function\n",
    "history = emb_model.fit(train_words_indexes, y_train, batch_size=128, epochs=1, validation_data=(val_words_indexes, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [x for x in range(len(history.history['auc']))]\n",
    "trace1 = {\"y\": history.history['auc'], \"x\": epochs, \"name\": \"train_auc\", \"type\": \"scatter\"}\n",
    "trace2 = {\"y\": history.history['val_auc'], \"x\": epochs, \"name\": \"val_auc\", \"type\": \"scatter\"}\n",
    "trace3 = {\"y\": history.history['loss'], \"x\": epochs, \"name\": \"train_loss\", \"type\": \"scatter\"}\n",
    "trace4 = {\"y\": history.history['val_loss'], \"x\": epochs, \"name\": \"val_loss\", \"type\": \"scatter\"}\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "layout = {\"title\": \"Embeddings + MLP model\"}\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='embmlp-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained embeddings\n",
    "Training embeddings from scratch it's very time consuming and you may not have enough data to do so. \n",
    "\n",
    "Fortunately, nowadays you have a lot of pretrained word embeddings that you can use to train your model! Most of the times it is way easier to start your embeddings with ones that were previously trained on a large corpora. You can choose to freeze (not train) or finetune those embeddings in your model.  \n",
    "\n",
    "In this tutorial we will finetune the pretrained fastText embeddings but feel free to download any other english embeddings and try them out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the fastText embeddings [here](https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip), extract the file `crawl-300d-2M.vec` and place it in the root of this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now open the embeddings file and get the words and their corresponding vectors\n",
    "# and construct embedding matrix to pass to the model's embedding layer\n",
    "EMBEDDING_FILE = 'crawl-300d-2M.vec'\n",
    "embeddings_dim = 300\n",
    "embeddings_matrix = np.zeros((num_words, embeddings_dim))\n",
    "def get_coefs(word, *arr): \n",
    "    if word not in tokenizer.word_index or tokenizer.word_index[word] >= num_words:\n",
    "        return None\n",
    "    return (tokenizer.word_index[word], np.asarray(arr, dtype='float32')) \n",
    "with open(EMBEDDING_FILE) as fp:\n",
    "    for o in tqdm(fp, desc=\"Reading embeddings...\"):\n",
    "        aux = get_coefs(*o.rstrip().rsplit(' '))\n",
    "        if aux is not None:\n",
    "            embeddings_matrix[aux[0]] = aux[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some embeddings\n",
    "print(\"Word toxic:\")\n",
    "print(\"Id:\", tokenizer.word_index['toxic'])\n",
    "print(\"Embedding:\", embeddings_matrix[tokenizer.word_index['toxic']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pretrained_emb_model = keras_models.get_pretrained_embeddings_model(maxlen, num_words, embeddings_dim, embeddings_matrix)\n",
    "pretrained_emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check https://keras.io/models/model/#fit for more option on arguments to pass to the fit function\n",
    "history = pretrained_emb_model.fit(train_words_indexes, y_train, batch_size=128, epochs=5, validation_data=(val_words_indexes, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [x for x in range(len(history.history['auc']))]\n",
    "trace1 = {\"y\": history.history['auc'], \"x\": epochs, \"name\": \"train_auc\", \"type\": \"scatter\"}\n",
    "trace2 = {\"y\": history.history['val_auc'], \"x\": epochs, \"name\": \"val_auc\", \"type\": \"scatter\"}\n",
    "trace3 = {\"y\": history.history['loss'], \"x\": epochs, \"name\": \"train_loss\", \"type\": \"scatter\"}\n",
    "trace4 = {\"y\": history.history['val_loss'], \"x\": epochs, \"name\": \"val_loss\", \"type\": \"scatter\"}\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "layout = {\"title\": \"Pretrained Embeddings + MLP model\"}\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='pretrainedembmlp-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent networks\n",
    "We have been using a feedforward multilayer perceptron so far. However, we have learned that text representations are better handled by recurrent networks.\n",
    "\n",
    "![this5](https://www.researchgate.net/profile/Wim_De_mulder/publication/266204519/figure/fig5/AS:270318480654346@1441460356163/Recurrent-versus-feedforward-neural-network.png)\n",
    "\n",
    "In this assignment, you are challenged to follow the example of `keras_models.get_embeddings_model`, implement a **bidirectional recurrent** network and try to get a good score on the final Kaggle test (You'll have to load the data yourself, pass it through the tokenizer and call `keras_models.get_score_on_kaggle_test_set` on it). \n",
    "\n",
    "Go to `keras_models.py` and implement the `get_recurrent_model` function.\n",
    "\n",
    "Tip: Have a look at Keras' [documentation on recurrent layers](https://keras.io/layers/recurrent/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_model = keras_models.get_recurrent_model(maxlen, num_words, embeddings_dim)\n",
    "recurrent_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = recurrent_model.fit(train_words_indexes, y_train, batch_size=128, epochs=5, validation_data=(val_words_indexes, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [x for x in range(len(history.history['auc']))]\n",
    "trace1 = {\"y\": history.history['auc'], \"x\": epochs, \"name\": \"train_auc\", \"type\": \"scatter\"}\n",
    "trace2 = {\"y\": history.history['val_auc'], \"x\": epochs, \"name\": \"val_auc\", \"type\": \"scatter\"}\n",
    "trace3 = {\"y\": history.history['loss'], \"x\": epochs, \"name\": \"train_loss\", \"type\": \"scatter\"}\n",
    "trace4 = {\"y\": history.history['val_loss'], \"x\": epochs, \"name\": \"val_loss\", \"type\": \"scatter\"}\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "layout = {\"title\": \"Recurrent model\"}\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='recurrent-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Possible) solution:**\n",
    "\n",
    "```\n",
    "def get_recurrent_model(input_len, num_words, embeddings_dim):\n",
    "    # Inputs\n",
    "    input = Input(shape=(input_len,), name=\"comment_words_idx\")\n",
    "\n",
    "    # Embedding layer\n",
    "\n",
    "    x = Embedding(num_words+2, embeddings_dim)(input)\n",
    "\n",
    "    # Spatial droupout layer\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "    # Recurrent bi-directional layer\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "\n",
    "    # Flatten\n",
    "    x = Flatten(x)\n",
    "\n",
    "    # Classification head\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # output\n",
    "    output = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "    # model\n",
    "    model = Model([input], output)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer='adam', metrics=['accuracy', auc])\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Contextual embeddings\n",
    "Congratulations if you got through here!\n",
    "See if you can find a way to add ELMo, BERT or other contextual embeddings to your model. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
