{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def dataset_preparation(data):\n",
    "\n",
    "    # basic cleanup\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "    # tokenization    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # create input sequences using list of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # pad sequences \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # create predictors and label\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "    return predictors, label, max_sequence_len, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOM CASMURRO\n",
      "\n",
      "POR\n",
      "\n",
      "MACHADO DE ASSIS\n",
      "\n",
      "DA ACADEMIA BRAZILEIRA\n",
      "\n",
      "H. GARNIER, LIVREIRO-EDITOR\n",
      "\n",
      "RUA MOREIRA CEZAR, 71\n",
      "\n",
      "RIO DE JANEIRO\n",
      "\n",
      "6, RUE DES SAINTS-PÈRES, 6\n",
      "\n",
      "PARIZ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "Do titulo.\n",
      "\n",
      "Uma noite destas, vindo da cidade para o Engenho Novo, encontrei no\n",
      "trem da Central um rapaz aqui do bairro, que eu conheço de vista e\n",
      "de chapéo. Comprimentou-me, sentou-se ao pé de mim, falou da lua e\n",
      "dos ministros, e acabou recitando-me versos. A viagem era curta, e os\n",
      "versos póde ser que não fossem inteiramente maus. Succedeu, porém, que\n",
      "como eu estava cançado, fechei os olhos tres ou quatro vezes; tanto\n",
      "bastou para que elle interrompesse a leitura e mettesse os versos no\n",
      "bolso.\n",
      "\n",
      "--Continue, disse eu accordando.\n",
      "\n",
      "--Já acabei, murmurou elle.\n",
      "\n",
      "--São muito bonitos.\n",
      "\n",
      "Vi-lhe fazer um gesto para tiral-os outra vez do bolso, mas não passou\n",
      "do gesto; estava amuado. No dia seguinte entrou a dizer de mim nomes\n",
      "feios, e acabou alcunhando-me _Dom Casmurro._ Os visinhos, que não\n",
      "gostam dos meus habitos reclusos e calados, deram curso á alcunha, que\n",
      "afinal pegou. Nem por isso me zanguei. Contei a anecdota aos amigos da\n",
      "cidade, e elles, por graça, chamam-me assim, alguns em bilhetes: «Dom\n",
      "Casmurro, domingo vou jantar com você.»--«Vou para Petropolis, Dom\n",
      "Casmurro; a casa é a mesma da Rhenania; vê se deixas essa caverna do\n",
      "Engenho Novo, e vae lá passar uns quinze dias commigo.»--«Meu caro Dom\n",
      "Casmurro, não cuide que o dispenso do theatro amanhã; venha e dormirá\n",
      "aqui na cidade; dou-lhe camarote, dou-lhe chá, dou-lhe cama; só não lhe\n",
      "dou moça.»\n",
      "\n",
      "Não consultes diccionarios. _Casmurro_ não está aqui no sentido que\n",
      "elles lhe dão, mas no que lhe poz o vulgo de homem calado e mettido\n",
      "comsigo. _Dom_ veiu por ironia, para attribuir-me fumos de fidalgo.\n",
      "Tudo por estar cochilando! Tambem não achei melhor titulo para a minha\n",
      "narração; se não tiver outro d'aqui até ao fim do livro, vae este\n",
      "mesmo. O meu poeta do trem ficará sabendo que não lhe guardo rancor.\n",
      "E com pequeno esforço, sendo o titulo seu, poderá cuidar que a obra\n",
      "é sua. Ha livros que apenas terão isso dos seus autores; alguns nem\n",
      "tanto.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "II.\n",
      "\n",
      "Do livro.\n",
      "\n",
      "Agora que expliquei o titulo, passo a escrever o livro. Antes disso,\n",
      "porém, digamos os motivos que me põem a penna na mão.\n",
      "\n",
      "Vivo só, com um creado. A casa em que moro é propria; fil-a\n",
      "construir de proposito, levado de um desejo tão particular que me\n",
      "vexa imprimil-o, mas vá lá. Um dia, ha bastantes annos, lembrou-me\n",
      "reproduzir no Engenho Novo a casa em que me criei na antiga rua de\n",
      "Matacavallos, dando-lhe o mesmo aspecto e economia daquella outra,\n",
      "que desappareceu. Constructor e pintor entenderam bem as indicações\n",
      "que lhes fiz: é o mesmo predio assobradado, tres janellas de frente,\n",
      "varanda ao fundo, as mesmas alcovas e salas. Na principal destas, a\n",
      "pintura do tecto e das paredes é mais ou menos egual, umas grinaldas de\n",
      "flores miudas e grandes passaros que as tomam nos bicos, de espaço a\n",
      "espaço. Nos quatro cantos do tecto as figuras das estações, e ao centro\n",
      "das paredes os medalhões de Cesar, Augusto, Nero e Massinissa, com os\n",
      "nomes por baixo... Não alcanço a razão de taes personagens. Quando\n",
      "fomos para a casa de Matacavallos, já ella estava assim decorada; vinha\n",
      "do decennio anterior. Naturalmente era gosto do tempo metter sabor\n",
      "classico e figuras antigas em pinturas americanas. O mais é tambem\n",
      "analogo e parecido. Tenho chacarinha, flôres, legume, uma casuarina, um\n",
      "poço e lavadouro. Uso louça velha e mobilia velha. Emfim, agora, como\n",
      "outr'ora, ha aqui o mesmo contraste da vida interior, que é pacata, com\n",
      "a exterior, que é ruidosa.\n",
      "\n",
      "O meu fim evidente era atar as duas pontas da vida, e restaurar na\n",
      "velhice a adolescencia. Pois, senhor, não consegui recompor o que foi\n",
      "nem o que fui. Em tudo, se o rosto é egual, a physionomia é differente.\n",
      "Se só me faltassem os outros, vá; um homem consola-se mais ou menos\n",
      "das pessoas que perde; mas falto eu mesmo, e esta lacuna é tudo. O que\n",
      "aqui está é, mal comparando, semelhante á pintura que se põe na barba e\n",
      "nos cabellos, e que apenas conserva o habito externo, como se diz nas\n",
      "autopsias; o interno não aguenta tinta. Uma certidão que me desse vinte\n",
      "annos de edade poderia enganar os extranhos, como todos os documentos\n",
      "falsos, mas não a mim. Os amigos que me restam são de data recente;\n",
      "todos os antigos foram estudar a geologia dos campos santos. Quanto ás\n",
      "amigas, algumas datam de quinze annos, outras de menos, e quasi todas\n",
      "creem na mocidade. Duas ou tres fariam crer nella aos outros, mas a\n",
      "lingua que falam obriga muita vez a consultar os diccionarios, e tal\n",
      "frequencia é cançativa.\n",
      "\n",
      "Entretanto, vida differente não quer dizer vida peor; é outra cousa.\n",
      "A certos respeitos, aquella vida antiga apparece-me despida de muitos\n",
      "encantos que lhe achei; mas é tambem exacto que perdeu muito espinho\n",
      "que a fez molesta, e, de memoria, conservo alguma recordação doce e\n",
      "feiticeira. Em verdade, pouco appareco e menos falo. Distracções raras.\n",
      "O mais do tempo é gasto em hortar, jardinar e ler; como bem e não durmo\n",
      "mal.\n",
      "\n",
      "Ora, como tudo cança, esta monotonia acabou por exhaurir-me tambem.\n",
      "Quiz variar, e lembrou-me escrever um livro. Jurisprudencia,\n",
      "philosophia e politica acudiram-me, mas não me acudiram as forças\n",
      "necessarias. Depois, pensei em fazer uma _Historia dos Suburbios_,\n",
      "menos secca que as memorias do padre Luiz Gonçalves dos Santos,\n",
      "relativas á cidade; era obra modesta, mas exigia documentos e datas,\n",
      "como preliminares, tudo arido e longo. Foi então que os bustos pintados\n",
      "nas paredes entraram a falar-me e a dizer-me que, uma vez que elles não\n",
      "alcançavam reconstituir-me os tempos idos, pegasse da penna e contasse\n",
      "alguns. Talvez a narração me désse a illusão, e as sombras viessem\n",
      "perpassar ligeiras, como ao poeta, não o do trem, mas o do _Fausto: Ahi\n",
      "vindes outra vez, inquietas sombras...?_\n",
      "\n",
      "Fiquei tão alegre com esta ideia, que ainda agora me treme a penna na\n",
      "mão. Sim, Nero, Augusto, Massinissa, e tu, grande Cesar, que me incitas\n",
      "a fazer os meus commentarios, agradeço-vos o conselho, e vou deitar ao\n",
      "papel as reminiscencias que me vierem vindo. Deste modo, viverei o que\n",
      "vivi, e assentarei a mão para alguma obra de maior tomo. Eia, comecemos\n",
      "a evocação por uma celebre tarde de Novembro, que nunca me esqueceu.\n",
      "Tive outras muitas, melhores, e peores, mas aquella nunca se me apagou\n",
      "do espirito. É o que vás entender, lendo.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "III\n",
      "\n",
      "A denuncia.\n",
      "\n",
      "Ia a entrar na sala de visitas, quando ouvi proferir o meu nome e\n",
      "escondi-me atraz da porta. A casa era a da rua de Matacavallos, o mez\n",
      "Novembro, o anno é que é um tanto remoto, mas eu não hei de trocar as\n",
      "datas á minha vida só para agradar ás pessoas que não amam historias\n",
      "velhas; o anno era de 1857.\n",
      "\n",
      "--D. Gloria, a senhora persiste na ideia de metter o nosso Bentinho no\n",
      "seminario? É mais que tempo, e já agora póde haver uma difficuldade.\n",
      "\n",
      "--Que difficuldade?\n",
      "\n",
      "--Uma grande difficuldade.\n",
      "\n",
      "Minha mãe quiz saber o que era. José Dias, depois de alguns instantes\n",
      "de concentrarão, veiu ver se havia alguem no corredor; não deu por mim,\n",
      "voltou e, abafando a voz, disse que a difficuldade estava na casa ao\n",
      "pé, a gente do Padua.\n",
      "\n",
      "--A gente do Padua?\n",
      "\n",
      "--Ha algum tempo estou para lhe dizer isto, mas não me atrevia. Não\n",
      "me parece bonito que o nosso Bentinho ande mettido nos cantos com a\n",
      "filha do _Tartaruga_, e esta é a difficuldade, porque se elles pegam de\n",
      "namoro, a senhora terá muito que lutar para separal-os.\n",
      "\n",
      "--Não acho. Mettidos nos cantos?\n",
      "\n",
      "--É um modo de falar. Em segredinhos, sempre juntos. Bentinho quasi\n",
      "que não sae de lá. A pequena é uma desmiolada; o pae faz que não vê;\n",
      "tomara elle que as cousas corressem de maneira, que... Comprehendo o\n",
      "seu gesto; a senhora não crê em taes calculos, parece-lhe que todos têm\n",
      "a alma candida...\n",
      "\n",
      "--Mas, Sr. José Dias, tenho visto os pequenos brincando, e nunca vi\n",
      "nada que faça desconfiar. Basta a edade; Bentinho mal tem quinze annos.\n",
      "Capitú fez quatorze á semana passada; são dous creançolas. Não se\n",
      "esqueça que foram criados juntos, desde aquella grande enchente, ha\n",
      "dez annos, em que a familia Padua perdeu tanta cousa; d'ahi vieram as\n",
      "nossas relações. Pois eu hei de crer...? Mano Cosme, você que acha?\n",
      "\n",
      "Tio Cosme respondeu com um «Ora!» que, traduzido em vulgar, queria\n",
      "dizer: «São imaginações do José Dias; os pequenos divertem-se, eu\n",
      "divirto-me; onde está o gamão?»\n",
      "\n",
      "--Sim, creio que o senhor está enganado.\n",
      "\n",
      "--Póde ser, minha senhora. Oxalá tenham razão; mas creia que não falei\n",
      "senão depois de muito examinar...\n",
      "\n",
      "--Em todo caso, vae sendo tempo, interrompeu minha mãe; vou tratar de\n",
      "mettel-o no seminario quanto antes.\n",
      "\n",
      "--Bem, uma vez que não perdeu a ideia de o fazer padre, tem-se ganho o\n",
      "principal. Bentinho ha de satisfazer os desejos de sua mãe. E depois\n",
      "a egreja brasileira tem altos destinos. Não esqueçamos que um bispo\n",
      "presidiu a Constituinte, e que o padre Feijó governou o imperio...\n",
      "\n",
      "--Governou como a cara d'elle! atalhou tio Cosme, cedendo a antigos\n",
      "rancores politicos.\n",
      "\n",
      "--Perdão, doutor, não estou defendendo ninguem, estou citando. O que eu\n",
      "quero é dizer que o clero ainda tem grande papel no Brasil.\n",
      "\n",
      "--Você o que quer é um capote; ande, vá buscar o gamão. Quanto ao\n",
      "pequeno, se tem de ser padre, realmente é melhor que não comece a dizer\n",
      "missa atraz das portas. Mas, olhe cá, mana Gloria, ha mesmo necessidade\n",
      "de fazel-o padre?\n",
      "\n",
      "--É promessa, ha de cumprir-se.\n",
      "\n",
      "--Sei que você fez promessa... mas, uma promessa assim... não sei...\n",
      "Creio que, bem pensado... Você que acha, prima Justina?\n",
      "\n",
      "--Eu?\n",
      "\n",
      "--Verdade é que cada um sabe melhor de si, continuou tio Cosme; Deus é\n",
      "que sabe do todos. Comtudo, uma promessa de tantos annos... Mas, que\n",
      "é isso, mana Gloria? Está chorando? Ora esta! Pois isto é cousa de\n",
      "lagrimas?\n",
      "\n",
      "Minha mãe assoou-se sem responder. Prima Justina creio que se levantou\n",
      "e foi ter com ella. Seguiu-se um alto silencio, durante o qual estive\n",
      "a pique de entrar na sala, mas outra força maior, outra emoção... Não\n",
      "pude ouvir as palavras que tio Cosme entrou a dizer. Prima Justina\n",
      "exhortava: «Prima Gloria! prima Gloria!» José Dias desculpava-se: «Se\n",
      "soubesse, não teria \n"
     ]
    }
   ],
   "source": [
    "cut_size = 10000\n",
    "data = open('datasets/DomCasmurro.txt').read()[:cut_size]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len, total_words = dataset_preparation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pedrobalage/Documents/portoai-nlp/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 15, 10)            7770      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 15, 150)           96600     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               100400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 777)               78477     \n",
      "=================================================================\n",
      "Total params: 283,247\n",
      "Trainable params: 283,247\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences = True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pedrobalage/Documents/portoai-nlp/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "1602/1602 [==============================] - 2s 1ms/step - loss: 6.3881 - acc: 0.0406\n",
      "Epoch 2/100\n",
      "1602/1602 [==============================] - 1s 572us/step - loss: 5.9572 - acc: 0.0456\n",
      "Epoch 3/100\n",
      "1602/1602 [==============================] - 1s 554us/step - loss: 5.8920 - acc: 0.0456\n",
      "Epoch 4/100\n",
      "1602/1602 [==============================] - 1s 572us/step - loss: 5.8751 - acc: 0.0456\n",
      "Epoch 5/100\n",
      "1602/1602 [==============================] - 1s 564us/step - loss: 5.8658 - acc: 0.0437\n",
      "Epoch 6/100\n",
      "1602/1602 [==============================] - 1s 577us/step - loss: 5.8592 - acc: 0.0456\n",
      "Epoch 7/100\n",
      "1602/1602 [==============================] - 1s 579us/step - loss: 5.8560 - acc: 0.0456\n",
      "Epoch 8/100\n",
      "1602/1602 [==============================] - 1s 565us/step - loss: 5.8428 - acc: 0.0362\n",
      "Epoch 9/100\n",
      "1602/1602 [==============================] - 1s 567us/step - loss: 5.8121 - acc: 0.0456\n",
      "Epoch 10/100\n",
      "1602/1602 [==============================] - 1s 572us/step - loss: 5.7479 - acc: 0.0456\n",
      "Epoch 11/100\n",
      "1602/1602 [==============================] - 1s 569us/step - loss: 5.6630 - acc: 0.0406\n",
      "Epoch 12/100\n",
      "1602/1602 [==============================] - 1s 567us/step - loss: 5.5715 - acc: 0.0449\n",
      "Epoch 13/100\n",
      "1602/1602 [==============================] - 1s 575us/step - loss: 5.5045 - acc: 0.0431\n",
      "Epoch 14/100\n",
      "1602/1602 [==============================] - 1s 571us/step - loss: 5.4562 - acc: 0.0418\n",
      "Epoch 15/100\n",
      "1602/1602 [==============================] - 1s 572us/step - loss: 5.4124 - acc: 0.0412\n",
      "Epoch 16/100\n",
      "1602/1602 [==============================] - 1s 567us/step - loss: 5.3714 - acc: 0.0387\n",
      "Epoch 17/100\n",
      "1602/1602 [==============================] - 1s 561us/step - loss: 5.3276 - acc: 0.0424\n",
      "Epoch 18/100\n",
      "1602/1602 [==============================] - 1s 645us/step - loss: 5.2751 - acc: 0.0456\n",
      "Epoch 19/100\n",
      "1602/1602 [==============================] - 1s 579us/step - loss: 5.2284 - acc: 0.0456\n",
      "Epoch 20/100\n",
      "1602/1602 [==============================] - 1s 561us/step - loss: 5.1851 - acc: 0.0512\n",
      "Epoch 21/100\n",
      "1602/1602 [==============================] - 1s 558us/step - loss: 5.1416 - acc: 0.0499\n",
      "Epoch 22/100\n",
      "1602/1602 [==============================] - 1s 564us/step - loss: 5.0960 - acc: 0.0462\n",
      "Epoch 23/100\n",
      "1602/1602 [==============================] - 1s 577us/step - loss: 5.0624 - acc: 0.0487\n",
      "Epoch 24/100\n",
      "1602/1602 [==============================] - 1s 591us/step - loss: 5.0109 - acc: 0.0481\n",
      "Epoch 25/100\n",
      "1602/1602 [==============================] - 1s 587us/step - loss: 4.9588 - acc: 0.0537\n",
      "Epoch 26/100\n",
      "1602/1602 [==============================] - 1s 600us/step - loss: 4.9176 - acc: 0.0562\n",
      "Epoch 27/100\n",
      "1602/1602 [==============================] - 1s 645us/step - loss: 4.8638 - acc: 0.0574\n",
      "Epoch 28/100\n",
      "1602/1602 [==============================] - 1s 632us/step - loss: 4.8193 - acc: 0.0537\n",
      "Epoch 29/100\n",
      "1602/1602 [==============================] - 1s 631us/step - loss: 4.7777 - acc: 0.0599\n",
      "Epoch 30/100\n",
      "1602/1602 [==============================] - 1s 638us/step - loss: 4.7411 - acc: 0.0587\n",
      "Epoch 31/100\n",
      "1602/1602 [==============================] - 1s 543us/step - loss: 4.6950 - acc: 0.0662\n",
      "Epoch 32/100\n",
      "1602/1602 [==============================] - 1s 562us/step - loss: 4.6619 - acc: 0.0680\n",
      "Epoch 33/100\n",
      "1602/1602 [==============================] - 1s 537us/step - loss: 4.6240 - acc: 0.0668\n",
      "Epoch 34/100\n",
      "1602/1602 [==============================] - 1s 539us/step - loss: 4.5848 - acc: 0.0743\n",
      "Epoch 35/100\n",
      "1602/1602 [==============================] - 1s 609us/step - loss: 4.5500 - acc: 0.0737\n",
      "Epoch 36/100\n",
      "1602/1602 [==============================] - 1s 537us/step - loss: 4.5070 - acc: 0.0724\n",
      "Epoch 37/100\n",
      "1602/1602 [==============================] - 1s 673us/step - loss: 4.4747 - acc: 0.0787\n",
      "Epoch 38/100\n",
      "1602/1602 [==============================] - 1s 677us/step - loss: 4.4360 - acc: 0.0755\n",
      "Epoch 39/100\n",
      "1602/1602 [==============================] - 1s 702us/step - loss: 4.4008 - acc: 0.0849\n",
      "Epoch 40/100\n",
      "1602/1602 [==============================] - 1s 583us/step - loss: 4.3682 - acc: 0.0799\n",
      "Epoch 41/100\n",
      "1602/1602 [==============================] - 1s 592us/step - loss: 4.3331 - acc: 0.0830\n",
      "Epoch 42/100\n",
      "1602/1602 [==============================] - 1s 587us/step - loss: 4.3101 - acc: 0.0718\n",
      "Epoch 43/100\n",
      "1602/1602 [==============================] - 1s 585us/step - loss: 4.2655 - acc: 0.0874\n",
      "Epoch 44/100\n",
      "1602/1602 [==============================] - 1s 596us/step - loss: 4.2300 - acc: 0.0843\n",
      "Epoch 45/100\n",
      "1602/1602 [==============================] - 1s 582us/step - loss: 4.1929 - acc: 0.0968\n",
      "Epoch 46/100\n",
      "1602/1602 [==============================] - 1s 631us/step - loss: 4.1649 - acc: 0.0886\n",
      "Epoch 47/100\n",
      "1602/1602 [==============================] - 1s 615us/step - loss: 4.1275 - acc: 0.0986\n",
      "Epoch 48/100\n",
      "1602/1602 [==============================] - 1s 586us/step - loss: 4.0960 - acc: 0.0974\n",
      "Epoch 49/100\n",
      "1602/1602 [==============================] - 1s 574us/step - loss: 4.0530 - acc: 0.1017\n",
      "Epoch 50/100\n",
      "1602/1602 [==============================] - 1s 577us/step - loss: 4.0319 - acc: 0.1005\n",
      "Epoch 51/100\n",
      "1602/1602 [==============================] - 1s 591us/step - loss: 3.9882 - acc: 0.1042\n",
      "Epoch 52/100\n",
      "1602/1602 [==============================] - 1s 594us/step - loss: 3.9616 - acc: 0.1074\n",
      "Epoch 53/100\n",
      "1602/1602 [==============================] - 1s 581us/step - loss: 3.9092 - acc: 0.1211\n",
      "Epoch 54/100\n",
      "1602/1602 [==============================] - 1s 616us/step - loss: 3.8702 - acc: 0.1192\n",
      "Epoch 55/100\n",
      "1602/1602 [==============================] - 1s 591us/step - loss: 3.8395 - acc: 0.1292\n",
      "Epoch 56/100\n",
      "1602/1602 [==============================] - 1s 587us/step - loss: 3.8044 - acc: 0.1261\n",
      "Epoch 57/100\n",
      "1602/1602 [==============================] - 1s 588us/step - loss: 3.7734 - acc: 0.1373\n",
      "Epoch 58/100\n",
      "1602/1602 [==============================] - 1s 589us/step - loss: 3.7461 - acc: 0.1336\n",
      "Epoch 59/100\n",
      "1602/1602 [==============================] - 1s 592us/step - loss: 3.7144 - acc: 0.1467\n",
      "Epoch 60/100\n",
      "1602/1602 [==============================] - 1s 592us/step - loss: 3.6810 - acc: 0.1467\n",
      "Epoch 61/100\n",
      "1602/1602 [==============================] - 1s 577us/step - loss: 3.6595 - acc: 0.1504\n",
      "Epoch 62/100\n",
      "1602/1602 [==============================] - 1s 590us/step - loss: 3.6224 - acc: 0.1542\n",
      "Epoch 63/100\n",
      "1602/1602 [==============================] - 1s 596us/step - loss: 3.5876 - acc: 0.1604\n",
      "Epoch 64/100\n",
      "1602/1602 [==============================] - 1s 586us/step - loss: 3.5665 - acc: 0.1673\n",
      "Epoch 65/100\n",
      "1602/1602 [==============================] - 1s 586us/step - loss: 3.5409 - acc: 0.1679\n",
      "Epoch 66/100\n",
      "1602/1602 [==============================] - 1s 603us/step - loss: 3.5378 - acc: 0.1692\n",
      "Epoch 67/100\n",
      "1602/1602 [==============================] - 1s 590us/step - loss: 3.4864 - acc: 0.1785\n",
      "Epoch 68/100\n",
      "1602/1602 [==============================] - 1s 666us/step - loss: 3.4447 - acc: 0.1923\n",
      "Epoch 69/100\n",
      "1602/1602 [==============================] - 1s 598us/step - loss: 3.4156 - acc: 0.1985\n",
      "Epoch 70/100\n",
      "1602/1602 [==============================] - 1s 583us/step - loss: 3.3939 - acc: 0.2060\n",
      "Epoch 71/100\n",
      "1602/1602 [==============================] - 1s 587us/step - loss: 3.3688 - acc: 0.2116\n",
      "Epoch 72/100\n",
      "1602/1602 [==============================] - 1s 586us/step - loss: 3.3375 - acc: 0.2122\n",
      "Epoch 73/100\n",
      "1602/1602 [==============================] - 1s 588us/step - loss: 3.3103 - acc: 0.2185\n",
      "Epoch 74/100\n",
      "1602/1602 [==============================] - 1s 593us/step - loss: 3.2762 - acc: 0.2222\n",
      "Epoch 75/100\n",
      "1602/1602 [==============================] - 1s 588us/step - loss: 3.2520 - acc: 0.2266\n",
      "Epoch 76/100\n",
      "1602/1602 [==============================] - 1s 585us/step - loss: 3.2197 - acc: 0.2341\n",
      "Epoch 77/100\n",
      "1602/1602 [==============================] - 1s 588us/step - loss: 3.1918 - acc: 0.2478\n",
      "Epoch 78/100\n",
      "1602/1602 [==============================] - 1s 590us/step - loss: 3.1805 - acc: 0.2391\n",
      "Epoch 79/100\n",
      "1602/1602 [==============================] - 1s 591us/step - loss: 3.1520 - acc: 0.2559\n",
      "Epoch 80/100\n",
      "1602/1602 [==============================] - 1s 579us/step - loss: 3.1218 - acc: 0.2628\n",
      "Epoch 81/100\n",
      "1602/1602 [==============================] - 1s 591us/step - loss: 3.0914 - acc: 0.2784\n",
      "Epoch 82/100\n",
      "1602/1602 [==============================] - 1s 594us/step - loss: 3.0655 - acc: 0.2859\n",
      "Epoch 83/100\n",
      "1602/1602 [==============================] - 1s 596us/step - loss: 3.0301 - acc: 0.2884\n",
      "Epoch 84/100\n",
      "1602/1602 [==============================] - 1s 592us/step - loss: 3.0005 - acc: 0.2928\n",
      "Epoch 85/100\n",
      "1602/1602 [==============================] - 1s 602us/step - loss: 2.9850 - acc: 0.3034\n",
      "Epoch 86/100\n",
      "1602/1602 [==============================] - 1s 594us/step - loss: 2.9560 - acc: 0.3152\n",
      "Epoch 87/100\n",
      "1602/1602 [==============================] - 1s 592us/step - loss: 2.9443 - acc: 0.3015\n",
      "Epoch 88/100\n",
      "1602/1602 [==============================] - 1s 662us/step - loss: 2.9172 - acc: 0.3134\n",
      "Epoch 89/100\n",
      "1602/1602 [==============================] - 1s 592us/step - loss: 2.8770 - acc: 0.3152\n",
      "Epoch 90/100\n",
      "1602/1602 [==============================] - 1s 627us/step - loss: 2.8593 - acc: 0.3427\n",
      "Epoch 91/100\n",
      "1602/1602 [==============================] - 1s 585us/step - loss: 2.8366 - acc: 0.3452\n",
      "Epoch 92/100\n",
      "1602/1602 [==============================] - 1s 588us/step - loss: 2.8049 - acc: 0.3427\n",
      "Epoch 93/100\n",
      "1602/1602 [==============================] - 1s 592us/step - loss: 2.7831 - acc: 0.3571\n",
      "Epoch 94/100\n",
      "1602/1602 [==============================] - 1s 603us/step - loss: 2.7583 - acc: 0.3489\n",
      "Epoch 95/100\n",
      "1602/1602 [==============================] - 1s 594us/step - loss: 2.7330 - acc: 0.3664\n",
      "Epoch 96/100\n",
      "1602/1602 [==============================] - 1s 603us/step - loss: 2.7082 - acc: 0.3745\n",
      "Epoch 97/100\n",
      "1602/1602 [==============================] - 1s 599us/step - loss: 2.6823 - acc: 0.3820\n",
      "Epoch 98/100\n",
      "1602/1602 [==============================] - 1s 592us/step - loss: 2.6652 - acc: 0.3970\n",
      "Epoch 99/100\n",
      "1602/1602 [==============================] - 1s 597us/step - loss: 2.6560 - acc: 0.3820\n",
      "Epoch 100/100\n",
      "1602/1602 [==============================] - 1s 612us/step - loss: 2.6328 - acc: 0.3983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd901774390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você o que quer é é lhe aspecto e acabou dias tenho visto os pequenos rua e mettido falo outras de menos guardo pegasse da\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Você o que quer é\"\n",
    "next_words = 20\n",
    "print(generate_text(seed_text, next_words, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
